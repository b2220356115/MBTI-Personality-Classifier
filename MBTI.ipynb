{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b6a5a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MBTI Personality Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9b6d9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4601278e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUNCTION : \n",
    "# We cannot process string values in ML, that's why we need to encode the last column with integers. Additionally, we will need string values when we finish tha classification...\n",
    "\"\"\"\n",
    "encoding_list = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "decoding_list = ['ESTJ','ENTJ','ESFJ','ENFJ','ISTJ','ISFJ','INTJ','INFJ','ESTP','ESFP','ENTP','ENFP','ISTP','ISFP','INTP','INFP']\n",
    "\n",
    "for i in encoding_list:\n",
    "    base_data.loc[base_data[\"Personality\"] == decoding_list[i], \"Personality\"] = i\n",
    "base_data.head()\n",
    "\"\"\"\n",
    "### crawl from TXT\n",
    "def encode_pers_types(df):\n",
    "    with open('16p-Mapping.txt') as f:\n",
    "        lines = f.readlines()\n",
    "    mappin = lines[-16:]\n",
    "\n",
    "    for i in range(len(mappin)):\n",
    "        pers_type = mappin[i][-5:-1]\n",
    "        pers_type_id = mappin[i][0:-6]\n",
    "        df.loc[df[\"Personality\"] == pers_type, \"Personality\"] = pers_type_id\n",
    "        \n",
    "    return base_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4543720a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'15'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('16p-Mapping.txt') as f:\n",
    "    lines = f.readlines()\n",
    "mappin = lines[-16:]\n",
    "\n",
    "for i in range(len(mappin)):\n",
    "    pers_type = mappin[i][-5:-1]\n",
    "    pers_type_id = mappin[i][0:-6]\n",
    "    print(pers_type_id)\n",
    "pers_type_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aeea06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUNCTION :\n",
    "## Feature Normalization\n",
    "# In order to have better results, we normalize the data\n",
    "\"\"\"\n",
    "# We can create a function to normalize all data in columns, then we can call this function for all columns.\n",
    "def normalize_column(df,column):\n",
    "    max_x = np.array(df[column].values.tolist()).max()\n",
    "    min_x = np.array(df[column].values.tolist()).min()\n",
    "    rang = max_x-min_x\n",
    "    for i in range(len(df[column])):\n",
    "        df.loc[i,column] = (df[column][i]-min_x)/rang    \n",
    "    return df\n",
    "# \n",
    "normalized_data = train_data\n",
    "for col in train_data.columns:\n",
    "    normalized_data = normalize_column(normalized_data,col)\n",
    "\"\"\"\n",
    "# While the previous method was okey for different dataframes but we have a dataframe almaost all columns has the same range and values (-3,-2,-1,0,1,2,3). \n",
    "# Therefore we can update the values manually for less computational effort\n",
    "def normalize_data(df):\n",
    "    value_list = [-3,-2,-1,0,1,2,3]\n",
    "    max_x = max(value_list)\n",
    "    min_x = min(value_list)\n",
    "    rang = max_x-min_x\n",
    "    normalized_value_list = list(map(lambda x: (x-min_x)/rang, value_list))\n",
    "    # in each column, for each cell we update to normalized value \n",
    "    for col in df.columns:\n",
    "        for i in range(len(value_list)):\n",
    "            df.loc[df[col] == value_list[i], col] = normalized_value_list[i]          \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb7d73e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUNCTION : \n",
    "\n",
    "def fold_dataframe(base_data,test_set_letter):\n",
    "    k_fold_number = 5\n",
    "    period = math.floor(len(base_data)/k_fold_number)\n",
    "    fold_A = base_data.iloc[:1*period+1,:]\n",
    "    fold_B = base_data.iloc[1*period+1:2*period+1,:]\n",
    "    fold_C = base_data.iloc[2*period+1:3*period+1,:]\n",
    "    fold_D = base_data.iloc[3*period+1:4*period+1,:]\n",
    "    fold_E = base_data.iloc[4*period+1:,:]\n",
    "    if test_set_letter == 'A':\n",
    "        folded_df = pd.concat([fold_E, fold_B, fold_C, fold_D, fold_A], axis=0)\n",
    "    elif test_set_letter == 'B':\n",
    "        folded_df = pd.concat([fold_A, fold_E, fold_C, fold_D, fold_B], axis=0)\n",
    "    elif test_set_letter == 'C':\n",
    "        folded_df = pd.concat([fold_A, fold_B, fold_E, fold_D, fold_C], axis=0)\n",
    "    elif test_set_letter == 'D':\n",
    "        folded_df = pd.concat([fold_A, fold_B, fold_C, fold_E, fold_D], axis=0)\n",
    "    elif test_set_letter == 'E':\n",
    "        folded_df = pd.concat([fold_A, fold_B, fold_C, fold_D, fold_E], axis=0)\n",
    "    return folded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29dad8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUNCTION : \n",
    "\n",
    "## Dataset partitioning\n",
    "def partition_df(base_data):\n",
    "    normalized_data = base_data\n",
    "    test_train_rate = 80/100\n",
    "    limit = math.floor(len(normalized_data)*test_train_rate)\n",
    "    train_x_data = normalized_data.iloc[0:limit+1,0:60]\n",
    "    train_y_data = normalized_data.iloc[0:limit+1,60:61]\n",
    "    test_x_data = normalized_data.iloc[limit+1:len(normalized_data),0:60]\n",
    "    test_y_data = normalized_data.iloc[limit+1:len(normalized_data),60:61]\n",
    "\n",
    "    ## dataframe to numpy array\n",
    "    train_x = train_x_data.to_numpy()\n",
    "    train_y = train_y_data.to_numpy()\n",
    "    test_x = test_x_data.to_numpy()\n",
    "    test_y = test_y_data.to_numpy()\n",
    "    return [train_x,train_y,test_x,test_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0607506",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUNCTION : \n",
    "\n",
    "### Distance Metric -trial1\n",
    "\"\"\"\n",
    "def calculate_distance(point1,point2):\n",
    "    dist = 0\n",
    "    for i in range(len(point1)):\n",
    "        dist = dist + (point1[i]-point2[i])**2\n",
    "    dist = dist**(1/2)\n",
    "    return dist\n",
    "# calculate_distance([0,0],[3,4]) it should return 5\n",
    "\"\"\"\n",
    "### Distance Metric -trial2\n",
    "\"\"\"\n",
    "def calculate_distances(point1,train_points):\n",
    "    # this function takes a points coordinates as a list and train points' coordinates as a matrix\n",
    "    # and return distance list\n",
    "    m= len(train_points)\n",
    "    point1_matrix = np.tile(point1,(m,1))\n",
    "    print(point1_matrix.shape)\n",
    "    # subtracting matrix\n",
    "    fark = train_points - point1_matrix\n",
    "    # dot product a1.b1 a2.b2 but this will create a1.b2 and a2.b1 therefore we will select only diagonal values\n",
    "    sqsum = np.dot(fark, fark.T)\n",
    "    # squareroot \n",
    "    dist = np.sqrt(sqsum).diagonal()\n",
    "    return dist\n",
    "# calculate_distances([0,0],[[3,4],[5,12]]) it should return a list as [5,13]\n",
    "\"\"\"\n",
    "### Distance Metric -trial3\n",
    "def calculate_distances(point1,train_points):\n",
    "    # this function takes a points coordinates as a list and train points' coordinates as a matrix\n",
    "    # and return distance list -previous method was again cause complexity issues\n",
    "    distances = np.linalg.norm(train_points - point1, axis=1) # distances between points can be computed with the norm of the difference vector\n",
    "    return distances\n",
    "# calculate_distances(np.array([0,0]),np.array([[3,4],[5,12]])) it should return a list as [5.,13.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5047386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUNCTION : \n",
    "\n",
    "### kNN classifier\n",
    "def knn_classify(train_x,train_y,test_x,k):\n",
    "\n",
    "    # We should decide how we assign class labels: Majority vote or plurality voting. \n",
    "    # There are 16 classes, therefore we will use plurality voting and required vote rate will be 100/16 % + = %6.25+\n",
    "    # What if there 2 options have rate greater than 6.25.. \n",
    "\n",
    "    # We will use Euclidean distance with p=2 as distance metric.\n",
    "    predicted_list = [] #np.zeros(len(test_x))\n",
    "    for test_row in test_x: # len(test_x) \n",
    "        #test_row = test_x[test_row_index:test_row_index+1][0]\n",
    "        dist_array = calculate_distances(test_row,train_x)\n",
    "        # we will find the nearest k neighbours and their target values then we will determine the class for the test row\n",
    "        # here we can use \"argsort\" also, however we will try to find them with loop\n",
    "        \"\"\"\n",
    "        for nn in range(k):\n",
    "            # getting the minimum value of distance list\n",
    "            min_val = min(dist_array)\n",
    "            # print(min_val)\n",
    "            #finding the index of minimum value \n",
    "            min_ind = dist_array.index(min_val)\n",
    "            # we will find the target value of this neighbour\n",
    "            tvalue = train_y[min_ind][0]\n",
    "            target_list.append(tvalue)\n",
    "            # we should update the dist_array value to a huge value so as to not select again\n",
    "            dist_array[min_ind] = max(dist_array)*99\n",
    "        \"\"\"\n",
    "        nearest_neighbors_indeces = dist_array.argsort()[:k]\n",
    "        nearest_neighbor_targets = train_y[nearest_neighbors_indeces]\n",
    "        target_list = [item for sublist in nearest_neighbor_targets for item in sublist]\n",
    "\n",
    "        # now, we need to find the most frequent element in the target value list\n",
    "        most_freq_val = max(set(target_list), key = target_list.count) # here, if there are different target values which have same frequency we will choose the first element - it may result in misperception\n",
    "        # print(target_list)\n",
    "        # predicted_list[test_row_index] = most_freq_val\n",
    "        predicted_list.append(most_freq_val)\n",
    "    # When we finish predictioning for each test row, we return the prediction results\n",
    "    return predicted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b279fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUNCTION : \n",
    "\n",
    "### Performance Evaluation Metrics Implementation\n",
    "def error_analysis(test_y,predicted_list):\n",
    "    misclassified_indeces = []\n",
    "    confusion_matrix = np.zeros([16,16], dtype = int)\n",
    "    for i in range(len(test_x)):\n",
    "        true_value = int(test_y[i][0])\n",
    "        prediction = int(predicted_list[i])\n",
    "        # finding misclassified samples \n",
    "        if true_value - prediction != 0:\n",
    "            misclassified_indeces.append(i)\n",
    "        # creating confusion matrix\n",
    "        confusion_matrix[prediction][true_value] = confusion_matrix[prediction][true_value] + 1\n",
    "    # There are 16 different classes, therefore we will calculate evaluation metrics for each class\n",
    "    evaluation_data = []\n",
    "    for ptype in range(16):\n",
    "        tp = confusion_matrix[ptype][ptype]\n",
    "        tn = sum(confusion_matrix.diagonal())-tp        \n",
    "        fp = sum(confusion_matrix[ptype])-tp\n",
    "        fn = sum(confusion_matrix[:][ptype])-tp\n",
    "\n",
    "        accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "        precision = (tp)/(tp+fp)\n",
    "        recall = (tp)/(tp+fn)\n",
    "        f1_score = 2*(precision*recall)/(precision+recall)\n",
    "        ptype_dict = {'ptype1':ptype,'accuracy':accuracy,'precision':precision,'recall':recall,'f1_score':f1_score}\n",
    "        evaluation_data.append(ptype_dict)\n",
    "    # To have a generic evaluation metric, we add total_score\n",
    "    total_tp = sum(confusion_matrix.diagonal())\n",
    "    total_fp = sum(sum(confusion_matrix))-total_tp\n",
    "    total_score = total_tp/(total_tp+total_fp)\n",
    "    # returnin detailed evaluation data, total score, number of misclassified samples and their indices\n",
    "    return {'total_score':total_score},evaluation_data,{'misclassification_amount':len(misclassified_indeces)},misclassified_indeces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a959cfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAIN PART \n",
    "\n",
    "### kNN Algorithm Framewrok is READY TO RUN\n",
    "## 0 Loading Data\n",
    "rowdata = pd.read_csv('16P.csv',encoding='cp1252')\n",
    "## 1 Shuffling Data\n",
    "shuffled_data= rowdata.sample(frac=1) # function used: .sample()\n",
    "## 2 Manipulating data\n",
    "# We need to drop “Response Id” column, because it has no effect on classification\n",
    "base_data = shuffled_data.drop(columns={'Response Id'})\n",
    "# We cannot process string values in ML, that's why we need to encode the last column with integers. Additionally, we will need string values when we finish tha classification...\n",
    "base_data = encode_pers_types(base_data) # function used: encode_pers_types()\n",
    "## 3 Feature normalization (We will move on either with normalization and without normalization)\n",
    "unnormalized_data = base_data\n",
    "normalized_data = base_data\n",
    "normalized_data.iloc[:,0:60] = normalize_data(normalized_data.iloc[:,0:60]) # function used: normalize_data()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a4b2502",
   "metadata": {},
   "source": [
    "### Error Analysis for Classification\n",
    "# For different k values: Effect of Neighbor Number\n",
    "# With normalization and without normalization: Effect of Normalization\n",
    "# kNN and Weighted kNN: Effect of Algorithm\n",
    "# For different K-Folds: Effect of K-fold\n",
    "# --> Report \"Accuracy, Precision and Recall\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07c0148a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR k=  1 , with normalization and K-Fold: E\n",
      "{'total_score': 0.962080173347779}\n"
     ]
    }
   ],
   "source": [
    "### k = 1 | Normalized | K-fold: E\n",
    "k = 1\n",
    "K_letter = 'E'\n",
    "input_data = normalized_data\n",
    "## 4 Fold study and selecting fold to test\n",
    "normalized_testE = fold_dataframe(normalized_data,K_letter) # function used: fold_dataframe()\n",
    "## 5 Partitioning\n",
    "[train_x,train_y,test_x,test_y] = partition_df(normalized_testE) # function used: partition_df()\n",
    "## 6 Classification \n",
    "predictions_for_normalized_testE = knn_classify(train_x,train_y,test_x,k)\n",
    "\n",
    "\n",
    "# 7 Error Analysis\n",
    "print(\"RESULTS FOR k= \", k, \", with normalization and K-Fold:\", K_letter)\n",
    "detailed_error_analysis = error_analysis(test_y,predictions_for_normalized_testE)\n",
    "print(detailed_error_analysis[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66144b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR k=  3 , with normalization and K-Fold: E\n",
      "{'total_score': 0.9788315692974414}\n"
     ]
    }
   ],
   "source": [
    "### k = 3 | Normalized | K-fold: E\n",
    "k = 3\n",
    "K_letter = 'E'\n",
    "input_data = normalized_data\n",
    "## 4 Fold study and selecting fold to test\n",
    "normalized_testE = fold_dataframe(normalized_data,K_letter) # function used: fold_dataframe()\n",
    "## 5 Partitioning\n",
    "[train_x,train_y,test_x,test_y] = partition_df(normalized_testE) # function used: partition_df()\n",
    "## 6 Classification \n",
    "predictions_for_normalized_testE_k3 = knn_classify(train_x,train_y,test_x,k)\n",
    "\n",
    "# 7 Error Analysis\n",
    "print(\"RESULTS FOR k= \", k, \", with normalization and K-Fold:\", K_letter)\n",
    "detailed_error_analysis = error_analysis(test_y,predictions_for_normalized_testE_k3)\n",
    "print(detailed_error_analysis[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6134025a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR k=  5 , with normalization and K-Fold: E\n",
      "{'total_score': 0.9829152429369115}\n"
     ]
    }
   ],
   "source": [
    "### k = 5 | Normalized | K-fold: E\n",
    "k = 5\n",
    "K_letter = 'E'\n",
    "input_data = normalized_data\n",
    "## 4 Fold study and selecting fold to test\n",
    "normalized_testE = fold_dataframe(normalized_data,K_letter) # function used: fold_dataframe()\n",
    "## 5 Partitioning\n",
    "[train_x,train_y,test_x,test_y] = partition_df(normalized_testE) # function used: partition_df()\n",
    "## 6 Classification \n",
    "predictions_for_normalized_testE_k5 = knn_classify(train_x,train_y,test_x,k)\n",
    "\n",
    "# 7 Error Analysis\n",
    "print(\"RESULTS FOR k= \", k, \", with normalization and K-Fold:\", K_letter)\n",
    "detailed_error_analysis = error_analysis(test_y,predictions_for_normalized_testE_k5)\n",
    "print(detailed_error_analysis[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ab1c987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR k=  7 , with normalization and K-Fold: E\n",
      "{'total_score': 0.9838319859988333}\n"
     ]
    }
   ],
   "source": [
    "### k = 7 | Normalized | K-fold: E\n",
    "k = 7\n",
    "K_letter = 'E'\n",
    "input_data = normalized_data\n",
    "## 4 Fold study and selecting fold to test\n",
    "normalized_testE = fold_dataframe(normalized_data,K_letter) # function used: fold_dataframe()\n",
    "## 5 Partitioning\n",
    "[train_x,train_y,test_x,test_y] = partition_df(normalized_testE) # function used: partition_df()\n",
    "## 6 Classification \n",
    "predictions_for_normalized_testE_k7 = knn_classify(train_x,train_y,test_x,k)\n",
    "\n",
    "# 7 Error Analysis\n",
    "print(\"RESULTS FOR k= \", k, \", with normalization and K-Fold:\", K_letter)\n",
    "detailed_error_analysis = error_analysis(test_y,predictions_for_normalized_testE_k7)\n",
    "print(detailed_error_analysis[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90cc7bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'misclassification_amount': 194}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[55,\n",
       " 113,\n",
       " 202,\n",
       " 408,\n",
       " 488,\n",
       " 502,\n",
       " 510,\n",
       " 530,\n",
       " 539,\n",
       " 543,\n",
       " 697,\n",
       " 727,\n",
       " 798,\n",
       " 827,\n",
       " 834,\n",
       " 890,\n",
       " 897,\n",
       " 930,\n",
       " 944,\n",
       " 954,\n",
       " 1048,\n",
       " 1067,\n",
       " 1070,\n",
       " 1112,\n",
       " 1149,\n",
       " 1262,\n",
       " 1299,\n",
       " 1304,\n",
       " 1329,\n",
       " 1418,\n",
       " 1437,\n",
       " 1569,\n",
       " 1614,\n",
       " 1643,\n",
       " 1671,\n",
       " 1704,\n",
       " 1730,\n",
       " 1740,\n",
       " 1892,\n",
       " 1932,\n",
       " 2034,\n",
       " 2047,\n",
       " 2071,\n",
       " 2193,\n",
       " 2241,\n",
       " 2319,\n",
       " 2324,\n",
       " 2325,\n",
       " 2334,\n",
       " 2427,\n",
       " 2468,\n",
       " 2478,\n",
       " 2479,\n",
       " 2482,\n",
       " 2536,\n",
       " 2565,\n",
       " 2644,\n",
       " 2669,\n",
       " 2755,\n",
       " 2778,\n",
       " 2824,\n",
       " 2935,\n",
       " 2940,\n",
       " 2942,\n",
       " 3133,\n",
       " 3224,\n",
       " 3411,\n",
       " 3442,\n",
       " 3515,\n",
       " 3543,\n",
       " 3706,\n",
       " 3764,\n",
       " 3768,\n",
       " 3805,\n",
       " 3839,\n",
       " 3855,\n",
       " 4011,\n",
       " 4064,\n",
       " 4269,\n",
       " 4276,\n",
       " 4367,\n",
       " 4443,\n",
       " 4502,\n",
       " 4622,\n",
       " 4640,\n",
       " 4650,\n",
       " 4663,\n",
       " 4771,\n",
       " 4840,\n",
       " 4847,\n",
       " 4920,\n",
       " 4963,\n",
       " 5141,\n",
       " 5150,\n",
       " 5308,\n",
       " 5499,\n",
       " 5510,\n",
       " 5928,\n",
       " 5993,\n",
       " 5998,\n",
       " 6033,\n",
       " 6150,\n",
       " 6218,\n",
       " 6221,\n",
       " 6263,\n",
       " 6288,\n",
       " 6324,\n",
       " 6343,\n",
       " 6385,\n",
       " 6431,\n",
       " 6436,\n",
       " 6469,\n",
       " 6577,\n",
       " 6620,\n",
       " 6647,\n",
       " 6651,\n",
       " 6994,\n",
       " 7030,\n",
       " 7036,\n",
       " 7094,\n",
       " 7101,\n",
       " 7108,\n",
       " 7182,\n",
       " 7408,\n",
       " 7551,\n",
       " 7610,\n",
       " 7641,\n",
       " 7679,\n",
       " 7688,\n",
       " 7694,\n",
       " 7807,\n",
       " 7870,\n",
       " 7871,\n",
       " 7872,\n",
       " 7906,\n",
       " 7963,\n",
       " 8082,\n",
       " 8106,\n",
       " 8125,\n",
       " 8158,\n",
       " 8331,\n",
       " 8339,\n",
       " 8357,\n",
       " 8396,\n",
       " 8398,\n",
       " 8416,\n",
       " 8451,\n",
       " 8513,\n",
       " 8811,\n",
       " 9068,\n",
       " 9124,\n",
       " 9126,\n",
       " 9153,\n",
       " 9167,\n",
       " 9218,\n",
       " 9228,\n",
       " 9262,\n",
       " 9426,\n",
       " 9476,\n",
       " 9595,\n",
       " 9605,\n",
       " 9697,\n",
       " 9778,\n",
       " 9787,\n",
       " 9879,\n",
       " 9909,\n",
       " 10015,\n",
       " 10091,\n",
       " 10348,\n",
       " 10365,\n",
       " 10422,\n",
       " 10555,\n",
       " 10584,\n",
       " 10624,\n",
       " 10654,\n",
       " 10877,\n",
       " 10892,\n",
       " 10923,\n",
       " 10927,\n",
       " 10964,\n",
       " 10975,\n",
       " 11083,\n",
       " 11097,\n",
       " 11106,\n",
       " 11232,\n",
       " 11342,\n",
       " 11451,\n",
       " 11495,\n",
       " 11516,\n",
       " 11544,\n",
       " 11553,\n",
       " 11819,\n",
       " 11821,\n",
       " 11933]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### number of misclassified samples and their indices...WHY?\n",
    "print(detailed_error_analysis[2])\n",
    "detailed_error_analysis[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c6dfd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR k=  9 , with normalization and K-Fold: E\n",
      "{'total_score': 0.9843320276689724}\n"
     ]
    }
   ],
   "source": [
    "### k = 9 | Normalized | K-fold: E\n",
    "k = 9\n",
    "K_letter = 'E'\n",
    "input_data = normalized_data\n",
    "## 4 Fold study and selecting fold to test\n",
    "normalized_testE = fold_dataframe(normalized_data,K_letter) # function used: fold_dataframe()\n",
    "## 5 Partitioning\n",
    "[train_x,train_y,test_x,test_y] = partition_df(normalized_testE) # function used: partition_df()\n",
    "## 6 Classification \n",
    "predictions_for_normalized_testE_k9 = knn_classify(train_x,train_y,test_x,k)\n",
    "\n",
    "\n",
    "# 7 Error Analysis\n",
    "print(\"RESULTS FOR k= \", k, \", with normalization and K-Fold:\", K_letter)\n",
    "detailed_error_analysis = error_analysis(test_y,predictions_for_normalized_testE_k9)\n",
    "print(detailed_error_analysis[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f407d77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR k=  7 , with normalization and K-Fold: E\n",
      "{'total_score': 0.9838319859988333}\n"
     ]
    }
   ],
   "source": [
    "### k = 7 | Normalized | K-fold: D\n",
    "k = 7\n",
    "K_letter = 'E'\n",
    "input_data = normalized_data\n",
    "## 4 Fold study and selecting fold to test\n",
    "normalized_testE = fold_dataframe(normalized_data,K_letter) # function used: fold_dataframe()\n",
    "## 5 Partitioning\n",
    "[train_x,train_y,test_x,test_y] = partition_df(normalized_testE) # function used: partition_df()\n",
    "## 6 Classification \n",
    "predictions_for_normalized_testD_k7 = knn_classify(train_x,train_y,test_x,k)\n",
    "\n",
    "# 7 Error Analysis\n",
    "print(\"RESULTS FOR k= \", k, \", with normalization and K-Fold:\", K_letter)\n",
    "detailed_error_analysis = error_analysis(test_y,predictions_for_normalized_testD_k7)\n",
    "print(detailed_error_analysis[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f376b48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR k=  7 , with normalization and K-Fold: C\n",
      "{'total_score': 0.9830819234936244}\n"
     ]
    }
   ],
   "source": [
    "### k = 7 | Normalized | K-fold: C\n",
    "k = 7\n",
    "K_letter = 'C'\n",
    "input_data = normalized_data\n",
    "## 4 Fold study and selecting fold to test\n",
    "normalized_testE = fold_dataframe(normalized_data,K_letter) # function used: fold_dataframe()\n",
    "## 5 Partitioning\n",
    "[train_x,train_y,test_x,test_y] = partition_df(normalized_testE) # function used: partition_df()\n",
    "## 6 Classification \n",
    "predictions_for_normalized_testC_k7 = knn_classify(train_x,train_y,test_x,k)\n",
    "\n",
    "# 7 Error Analysis\n",
    "print(\"RESULTS FOR k= \", k, \", with normalization and K-Fold:\", K_letter)\n",
    "detailed_error_analysis = error_analysis(test_y,predictions_for_normalized_testC_k7)\n",
    "print(detailed_error_analysis[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5dbf6ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR k=  7 , with normalization and K-Fold: B\n",
      "{'total_score': 0.9829152429369115}\n"
     ]
    }
   ],
   "source": [
    "### k = 7 | Normalized | K-fold: B\n",
    "k = 7\n",
    "K_letter = 'B'\n",
    "input_data = normalized_data\n",
    "## 4 Fold study and selecting fold to test\n",
    "normalized_testE = fold_dataframe(normalized_data,K_letter) # function used: fold_dataframe()\n",
    "## 5 Partitioning\n",
    "[train_x,train_y,test_x,test_y] = partition_df(normalized_testE) # function used: partition_df()\n",
    "## 6 Classification \n",
    "predictions_for_normalized_testB_k7 = knn_classify(train_x,train_y,test_x,k)\n",
    "\n",
    "\n",
    "# 7 Error Analysis\n",
    "print(\"RESULTS FOR k= \", k, \", with normalization and K-Fold:\", K_letter)\n",
    "detailed_error_analysis = error_analysis(test_y,predictions_for_normalized_testB_k7)\n",
    "print(detailed_error_analysis[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15b679a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR k=  7 , with normalization and K-Fold: A\n",
      "{'total_score': 0.9824152012667722}\n"
     ]
    }
   ],
   "source": [
    "### k = 7 | Normalized | K-fold: A\n",
    "k = 7\n",
    "K_letter = 'A'\n",
    "input_data = normalized_data\n",
    "## 4 Fold study and selecting fold to test\n",
    "normalized_testE = fold_dataframe(normalized_data,K_letter) # function used: fold_dataframe()\n",
    "## 5 Partitioning\n",
    "[train_x,train_y,test_x,test_y] = partition_df(normalized_testE) # function used: partition_df()\n",
    "## 6 Classification \n",
    "predictions_for_normalized_testA_k7 = knn_classify(train_x,train_y,test_x,k)\n",
    "\n",
    "# 7 Error Analysis\n",
    "print(\"RESULTS FOR k= \", k, \", with normalization and K-Fold:\", K_letter)\n",
    "detailed_error_analysis = error_analysis(test_y,predictions_for_normalized_testA_k7)\n",
    "print(detailed_error_analysis[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "140fdd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR k=  7 , with normalization and K-Fold: E\n",
      "{'total_score': 0.9838319859988333}\n"
     ]
    }
   ],
   "source": [
    "### k = 7 | UNNormalized | K-fold: E\n",
    "k = 7\n",
    "K_letter = 'E'\n",
    "input_data = unnormalized_data\n",
    "## 4 Fold study and selecting fold to test\n",
    "normalized_testE = fold_dataframe(input_data,K_letter) # function used: fold_dataframe()\n",
    "## 5 Partitioning\n",
    "[train_x,train_y,test_x,test_y] = partition_df(normalized_testE) # function used: partition_df()\n",
    "## 6 Classification \n",
    "predictions_for_unnormalized_testE = knn_classify(train_x,train_y,test_x,k)\n",
    "\n",
    "# 7 Error Analysis\n",
    "print(\"RESULTS FOR k= \", k, \", with normalization and K-Fold:\", K_letter)\n",
    "detailed_error_analysis = error_analysis(test_y,predictions_for_unnormalized_testE)\n",
    "print(detailed_error_analysis[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152d6908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7aa67309",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### BONUS PART\n",
    "## FUNCTION : \n",
    "\n",
    "### Weighted kNN classifier\n",
    "def weighted_knn_classify(train_x,train_y,test_x,k):\n",
    "\n",
    "    # We should decide how we assign class labels: Majority vote or plurality voting. \n",
    "    # There are 16 classes, therefore we will use plurality voting and required vote rate will be 100/16 % + = %6.25+\n",
    "    # What if there 2 options have rate greater than 6.25.. \n",
    "\n",
    "    # We will use Euclidean distance with p=2 as distance metric.\n",
    "    predicted_list = [] #np.zeros(len(test_x))\n",
    "    for test_row in test_x: # len(test_x) \n",
    "        #test_row = test_x[test_row_index:test_row_index+1][0]\n",
    "        dist_array = calculate_distances(test_row,train_x)\n",
    "        # we will find the nearest k neighbours and their target values then we will determine the class for the test row\n",
    "        # here we can use \"argsort\" also, however we will try to find them with loop\n",
    "        \"\"\"\n",
    "        for nn in range(k):\n",
    "            # getting the minimum value of distance list\n",
    "            min_val = min(dist_array)\n",
    "            # print(min_val)\n",
    "            #finding the index of minimum value \n",
    "            min_ind = dist_array.index(min_val)\n",
    "            # we will find the target value of this neighbour\n",
    "            tvalue = train_y[min_ind][0]\n",
    "            target_list.append(tvalue)\n",
    "            # we should update the dist_array value to a huge value so as to not select again\n",
    "            dist_array[min_ind] = max(dist_array)*99\n",
    "        \"\"\"\n",
    "        nearest_neighbors_indeces = dist_array.argsort()[:k]\n",
    "        nearest_neighbor_targets = train_y[nearest_neighbors_indeces]\n",
    "        target_list = [item for sublist in nearest_neighbor_targets for item in sublist]\n",
    "        ### WEIGHTED KNN ALGO SPECIFIC\n",
    "        distinct_targets = list(set(target_list))\n",
    "        weighted_list = []\n",
    "        for class_i_index in range(len(distinct_targets)):\n",
    "            item = distinct_targets[class_i_index]\n",
    "            class_i_indexes = [i for i, x in enumerate(target_list) if x == item]\n",
    "            weight = 0\n",
    "            for indices_of_targets in class_i_indexes:\n",
    "                distance_of_the_row = dist_array[nearest_neighbors_indeces[indices_of_targets]] # dist_array's j'th value | j is nearest_neighbors_indeces's 'indices_of_targets'th value \n",
    "                weight = weight + 1/(distance_of_the_row**2)\n",
    "            weighted_list.append(weight)\n",
    "        weighted_list = np.array(weighted_list)\n",
    "        prediction_class = distinct_targets[weighted_list.argsort()[:1][0]]\n",
    "        # now, we need to find the most frequent element in the target value list\n",
    "        most_freq_val = prediction_class\n",
    "        # predicted_list[test_row_index] = most_freq_val\n",
    "        predicted_list.append(most_freq_val)\n",
    "    # When we finish predictioning for each test row, we return the prediction results\n",
    "    return predicted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a3e02e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR k=  7 , with normalization and K-Fold: E\n",
      "{'total_score': 0.8061505125427119}\n"
     ]
    }
   ],
   "source": [
    "### k = 7 | Normalized | K-fold: E | WEIGHTED KNN\n",
    "k = 7\n",
    "K_letter = 'E'\n",
    "input_data = normalized_data\n",
    "## 4 Fold study and selecting fold to test\n",
    "normalized_testE = fold_dataframe(input_data,K_letter) # function used: fold_dataframe()\n",
    "## 5 Partitioning\n",
    "[train_x,train_y,test_x,test_y] = partition_df(normalized_testE) # function used: partition_df()\n",
    "## 6 Classification \n",
    "predictions_for_unnormalized_testE = weighted_knn_classify(train_x,train_y,test_x,k)\n",
    "\n",
    "# 7 Error Analysis\n",
    "print(\"RESULTS FOR k= \", k, \", with normalization and K-Fold:\", K_letter)\n",
    "detailed_error_analysis = error_analysis(test_y,predictions_for_unnormalized_testE)\n",
    "print(detailed_error_analysis[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7969c43c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de061db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
